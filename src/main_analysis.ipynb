{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Dependencies\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SciKit Learn\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.manifold import MDS, Isomap, TSNE\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Viz\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From http://archive.ics.uci.edu/ml/datasets/Heart+Disease\n",
    "# http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/heart-disease.names\n",
    "\n",
    "\n",
    "# Using the American data sets within these - the data from Cleveland and from Long Beach \n",
    "heart_df_cl = pd.read_csv(\"../data/processed.cleveland.data.txt\", header = None)\n",
    "#heart_df_lb = pd.read_csv(\"../data/processed.va.data.txt\", header = None)\n",
    "#heart_df_h = pd.read_csv(\"../data/processed.hungarian.data.txt\", header = None)\n",
    "#heart_df_sw = pd.read_csv(\"../data/processed.switzerland.data.txt\", header = None)\n",
    "\n",
    "#heart_df = heart_df_cl.append(heart_df_lb).append(heart_df_h).append(heart_df_sw)\n",
    "    \n",
    "heart_df = heart_df_cl\n",
    "# More interpretable names taken from dataset description above\n",
    "var_names = [\"age\", \"sex\", \"chest_pain\", \"rest_bp\", \"chol\", \"bld_gluc\",\\\n",
    "             \"ecg_st_e\",\"strs_max_hr\", \"ex_ang\", \"ecg_st_d_exer\", \"ecg_st_slope\",\\\n",
    "            \"cor_a\", \"strs_test\", \"diagnosis\"]\n",
    "\n",
    "heart_df.columns = var_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations: 303\n",
      "Features: 14\n"
     ]
    }
   ],
   "source": [
    "# Pre-Wrangled Data\n",
    "print(\"Observations: {}\".format(heart_df.shape[0]))\n",
    "print(\"Features: {}\".format(heart_df.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Predictors \n",
    "\n",
    "**Continuous Predictors**    \n",
    "`age`: Age  \n",
    "`rest_bp`: Resting Blood Pressure on Admission  \n",
    "`chol`: Serum Cholesterol Level (mg/dL)  \n",
    "`strs_max_hr`: Maximum Heart Rate achieved during Stress Testing  \n",
    "`ecg_st_d_exer`: ST depression induced by exercise relative to rest  \n",
    "\n",
    "**Categorical Predictors**    \n",
    "`sex`: Male or Female  \n",
    "`chest_pain`: Classified into 4 types of chest pain (where 1 is most highly correlated with cardiovascular) - therefore will need to be changed with dummy variables.   \n",
    "        -- Value 1: typical angina  \n",
    "        -- Value 2: atypical angina  \n",
    "        -- Value 3: non-anginal pain  \n",
    "        -- Value 4: asymptomatic  \n",
    "`bld_gluc`: Yes or No to whether the blood glucose is high (above 120mg/dL - i.e. 6.7 mmol/L)  \n",
    "`ecg_st_e`: ST Elevation on ECG at Rest. Value 1 means ST Elevation (highly indicative of coronary artery involvement), whereas value 2 is indicative of Left Ventricular Hypertrophy (which is less indicative but important). \n",
    "`ex_ang`: Yes or no to exercise-induced angina (chest pain caused from exercise)   \n",
    "`ecg_st_slope`: upsloping, downsloping, or flat ST segment during exercise  \n",
    "`cor_a`: Coronary artery vessels coloured by fluoroscopy (number of vessels from 0-3).   \n",
    "`strs_test`: Result of nuclear stress testing - only 3 levels. Where 6 means a fixed defect, 7 means a reversible defect, and 3 means normal.    \n",
    "\n",
    "\n",
    "Since some of these categorical predictors have more than 2 levels they will need to be altered as dummy variables. These include: `chest_pain`, `ecg_st_e`, `cor_a`, `strs_test`.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all observations with NAs\n",
    "heart_df = heart_df.replace(\"?\", np.nan)\n",
    "heart_df = heart_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: by Dropping NAs, I am introducing bias. It is possible that patients in higher acuity (i.e. sicker) will have had less opportunity to get all the testing before angiography -- therefore those that have no NAs are actually less likely to have a positive prediction (which denotes >50% occlusion on angiography. I understand this flaw, but for the purposes of this analysis I will only be looking at modeling those who are low acuity enough to have complete data, and with this additional constraint, I can then drop the NAs and will treat them as being missing completely at random.  \n",
    "\n",
    "This is less an issue when I am just looking at the cleveland data, because it is much more complete. The data from Hungary, Switzerland and Long Beach have much more NAs with regards to Stress Testing (`strs_test`, `ect_st_slope` - and this may make sense, these are expensive tests that you can't do on people who are high acuity (if someone is actively having a heart attack it's a bad idea to inject them with nucleotides and get them to run on a treadmill. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Predictor (Diagnosis) to 0 or 1, where 1 means >50% artery narrowing on angiography\n",
    "heart_df.diagnosis = [1 if x > 0 else x for x in heart_df.diagnosis]\n",
    "\n",
    "# Dataframe for later reference in visualization\n",
    "heart_df_original = heart_df\n",
    "\n",
    "# Convert the `chest_pain` 4 level categorical variable into 4 binary features\n",
    "heart_df = pd.get_dummies(heart_df, columns = [\"chest_pain\"])\n",
    "\n",
    "# Convert the `ecg_st_e` 3 level categorical variable into 3 binary features\n",
    "heart_df = pd.get_dummies(heart_df, columns = [\"ecg_st_e\"])\n",
    "\n",
    "# Convert the `cor_a` 4 level categorical variable into 4 binary features\n",
    "heart_df = pd.get_dummies(heart_df, columns = [\"cor_a\"])\n",
    "\n",
    "# Convert the `ecg_st_slope` 3 level categorical variable into 3 binary features\n",
    "heart_df = pd.get_dummies(heart_df, columns = [\"ecg_st_slope\"])\n",
    "\n",
    "# Convert the `strs_test` 3 level categorical variable into 3 binary features\n",
    "heart_df = pd.get_dummies(heart_df, columns = [\"strs_test\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>rest_bp</th>\n",
       "      <th>chol</th>\n",
       "      <th>bld_gluc</th>\n",
       "      <th>strs_max_hr</th>\n",
       "      <th>ex_ang</th>\n",
       "      <th>ecg_st_d_exer</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>chest_pain_1.0</th>\n",
       "      <th>...</th>\n",
       "      <th>cor_a_0.0</th>\n",
       "      <th>cor_a_1.0</th>\n",
       "      <th>cor_a_2.0</th>\n",
       "      <th>cor_a_3.0</th>\n",
       "      <th>ecg_st_slope_1.0</th>\n",
       "      <th>ecg_st_slope_2.0</th>\n",
       "      <th>ecg_st_slope_3.0</th>\n",
       "      <th>strs_test_3.0</th>\n",
       "      <th>strs_test_6.0</th>\n",
       "      <th>strs_test_7.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  sex  rest_bp   chol  bld_gluc  strs_max_hr  ex_ang  ecg_st_d_exer  \\\n",
       "0  63.0  1.0    145.0  233.0       1.0        150.0     0.0            2.3   \n",
       "1  67.0  1.0    160.0  286.0       0.0        108.0     1.0            1.5   \n",
       "2  67.0  1.0    120.0  229.0       0.0        129.0     1.0            2.6   \n",
       "3  37.0  1.0    130.0  250.0       0.0        187.0     0.0            3.5   \n",
       "4  41.0  0.0    130.0  204.0       0.0        172.0     0.0            1.4   \n",
       "\n",
       "   diagnosis  chest_pain_1.0      ...        cor_a_0.0  cor_a_1.0  cor_a_2.0  \\\n",
       "0          0               1      ...                1          0          0   \n",
       "1          1               0      ...                0          0          0   \n",
       "2          1               0      ...                0          0          1   \n",
       "3          0               0      ...                1          0          0   \n",
       "4          0               0      ...                1          0          0   \n",
       "\n",
       "   cor_a_3.0  ecg_st_slope_1.0  ecg_st_slope_2.0  ecg_st_slope_3.0  \\\n",
       "0          0                 0                 0                 1   \n",
       "1          1                 0                 1                 0   \n",
       "2          0                 0                 1                 0   \n",
       "3          0                 0                 0                 1   \n",
       "4          0                 1                 0                 0   \n",
       "\n",
       "   strs_test_3.0  strs_test_6.0  strs_test_7.0  \n",
       "0              0              1              0  \n",
       "1              1              0              0  \n",
       "2              0              0              1  \n",
       "3              1              0              0  \n",
       "4              1              0              0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations: 297\n",
      "Features: 25, with 1 Response Variable\n"
     ]
    }
   ],
   "source": [
    "# After-Wrangled Data\n",
    "print(\"Observations: {}\".format(heart_df.shape[0]))\n",
    "print(\"Features: {}, with {} Response Variable\".format(heart_df.shape[1]-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get into matrix states for next phase\n",
    "\n",
    "X = np.array(heart_df.drop([\"diagnosis\"], axis = 1))\n",
    "y = np.array(heart_df.diagnosis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additionally, I want to scale the continuous variables to be between 0 and 1, so that they don't overpower\n",
    "# the categorical variables\n",
    "\n",
    "scale_0_1 = MinMaxScaler((0,1))\n",
    "\n",
    "X = scale_0_1.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.80, random_state = 123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x11e3fe1d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11adc28d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "isomap = Isomap(n_components=2)\n",
    "Z_isomap = isomap.fit_transform(X)\n",
    "Z_df = pd.DataFrame(Z_isomap)\n",
    "Z_df.columns = [\"Isomap 1\", \"Isomap 2\"]\n",
    "Z_df[\"Response\"] = y\n",
    "\n",
    "sns.stripplot(x = \"Isomap 1\", y = \"Isomap 2\", hue = \"Response\", data = Z_df)\n",
    "plt.title(\"Exploring Clustering based on Isomap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this Isomap vis there does appear to be significant differences between the negative and positive responses, which is encouraging that a model will be able to separate the two. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset dataframes into those that resulted in >50% occlusion of coronary arteries on angiography and those that didn't\n",
    "heart_df_positive = heart_df.loc[heart_df[\"diagnosis\"] == 1,:].drop([\"diagnosis\"], axis = 1)\n",
    "heart_df_negative = heart_df.loc[heart_df[\"diagnosis\"] == 0,:].drop([\"diagnosis\"], axis = 1)\n",
    "\n",
    "# Get summary statistics for all the features in each dataframe\n",
    "heart_pos_describe = heart_df_positive.describe().loc[[\"mean\", \"min\",\"max\", \"50%\"],:]\n",
    "heart_neg_describe = heart_df_negative.describe().loc[[\"mean\", \"min\",\"max\", \"50%\"],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>50%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>4.115374</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sex</th>\n",
       "      <td>0.261268</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rest_bp</th>\n",
       "      <td>5.460036</td>\n",
       "      <td>6.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chol</th>\n",
       "      <td>8.360265</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-155.0</td>\n",
       "      <td>17.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bld_gluc</th>\n",
       "      <td>0.002235</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>strs_max_hr</th>\n",
       "      <td>-19.471761</td>\n",
       "      <td>-25.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>-19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ex_ang</th>\n",
       "      <td>0.396396</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ecg_st_d_exer</th>\n",
       "      <td>0.990301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chest_pain_1.0</th>\n",
       "      <td>-0.048905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chest_pain_2.0</th>\n",
       "      <td>-0.184307</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chest_pain_3.0</th>\n",
       "      <td>-0.274863</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chest_pain_4.0</th>\n",
       "      <td>0.508075</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ecg_st_e_0.0</th>\n",
       "      <td>-0.173540</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ecg_st_e_1.0</th>\n",
       "      <td>0.015648</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ecg_st_e_2.0</th>\n",
       "      <td>0.157892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cor_a_0.0</th>\n",
       "      <td>-0.477783</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cor_a_1.0</th>\n",
       "      <td>0.189918</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cor_a_2.0</th>\n",
       "      <td>0.182527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cor_a_3.0</th>\n",
       "      <td>0.105338</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ecg_st_slope_1.0</th>\n",
       "      <td>-0.380976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ecg_st_slope_2.0</th>\n",
       "      <td>0.349635</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ecg_st_slope_3.0</th>\n",
       "      <td>0.031341</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>strs_test_3.0</th>\n",
       "      <td>-0.523677</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>strs_test_6.0</th>\n",
       "      <td>0.050091</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>strs_test_7.0</th>\n",
       "      <td>0.473586</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       mean   min    max   50%\n",
       "age                4.115374   6.0    1.0   6.0\n",
       "sex                0.261268   0.0    0.0   0.0\n",
       "rest_bp            5.460036   6.0   20.0   0.0\n",
       "chol               8.360265   5.0 -155.0  17.5\n",
       "bld_gluc           0.002235   0.0    0.0   0.0\n",
       "strs_max_hr      -19.471761 -25.0   -7.0 -19.0\n",
       "ex_ang             0.396396   0.0    0.0   1.0\n",
       "ecg_st_d_exer      0.990301   0.0    2.0   1.2\n",
       "chest_pain_1.0    -0.048905   0.0    0.0   0.0\n",
       "chest_pain_2.0    -0.184307   0.0    0.0   0.0\n",
       "chest_pain_3.0    -0.274863   0.0    0.0   0.0\n",
       "chest_pain_4.0     0.508075   0.0    0.0   1.0\n",
       "ecg_st_e_0.0      -0.173540   0.0    0.0  -1.0\n",
       "ecg_st_e_1.0       0.015648   0.0    0.0   0.0\n",
       "ecg_st_e_2.0       0.157892   0.0    0.0   1.0\n",
       "cor_a_0.0         -0.477783   0.0    0.0  -1.0\n",
       "cor_a_1.0          0.189918   0.0    0.0   0.0\n",
       "cor_a_2.0          0.182527   0.0    0.0   0.0\n",
       "cor_a_3.0          0.105338   0.0    0.0   0.0\n",
       "ecg_st_slope_1.0  -0.380976   0.0    0.0  -1.0\n",
       "ecg_st_slope_2.0   0.349635   0.0    0.0   1.0\n",
       "ecg_st_slope_3.0   0.031341   0.0    0.0   0.0\n",
       "strs_test_3.0     -0.523677   0.0    0.0  -1.0\n",
       "strs_test_6.0      0.050091   0.0    0.0   0.0\n",
       "strs_test_7.0      0.473586   0.0    0.0   1.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return the Difference between each data frame summary statistics \n",
    "## Where positive numbers means more in the with coronary's blocked category (positive)\n",
    "heart_diff = (heart_pos_describe - heart_neg_describe).T\n",
    "heart_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table Discussion:**  \n",
    "Here I've kept together the continuous and the categorical features. Since all the categorical features are dummy variables, we can take a positive number to mean this feature is present more often in among the coronary disease 'positive' than the 'negative' (as determined by angiography).  \n",
    "\n",
    "Age, Sex, Resting Blood Pressure, and Cholesterol are all continuous variables that respond as I would expect - those with coronary artery (CA) disease will have higher for all of them because they are heart disease risk factors. As well, you would expect those without heart disease to be able to hit a higher max stress test (although this is a bit more variable. \n",
    "\n",
    "As for the categorical variables, to take a few examples it makes sense that `ex_ang` (exercise induced angina), Positive Stress test findings (`strs_test_6.0`, `strs_test_7.0`) and rest ECG ST segment findings (`ecg_st_slope_1.0` corresponding to ST elevation and `ecg_st_slope_2.0`) would be positive because they are more likely in patients with CA disease. Likewise Normal rest ECG (`ecg_st_slope_0.0`) and Normal Stress test (`ecg_test_3.0`) are both negative as I'd expect.  \n",
    "\n",
    "Some surprising relationships: even though every medical student knows that ST elevation means 'badness', there is only a very small difference between positive and negative CA disease for `ecg_st_e_1.0` of 0.015648 although I would have expected much higher - perhaps there are a lot of false positives among the non CA disease group. \n",
    "\n",
    "I also found it surprising that typical chest pain `chest_pain_1.0` (what you think of when you hear of someone having a heart attack) was actually more likely among those without CA disease, and asymptomatic `chest_pain_4.0` was most associated with CA disease. My hypothesis for this is that this dataset is *not high acuity*. The people who present to the emergency with a heart attack wouldn't have gotten the stress test like everyone in the above dataset has gotten. Therefore we should be thinking about this dataset as being the subpopulation of people with heart disease that are stable enough to get outpatient stress testing - this subpopulation may very well have more asymptomatic presentations.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Training Error is: 0.126582278481\n",
      "The Testing Error is: 0.116666666667\n"
     ]
    }
   ],
   "source": [
    "# First let's look at what plain logistic regression yields\n",
    "## Here I set the penalty to L2 and set the C arbitrarily large so that there is effectively no penalty\n",
    "log_model = LogisticRegression(penalty = \"l1\", C = 10000)\n",
    "log_model.fit(X_train, y_train)\n",
    "\n",
    "log_train = 1 - log_model.score(X_train, y_train)\n",
    "log_test = 1 - log_model.score(X_test, y_test)\n",
    "\n",
    "print(\"The Training Error is:\", log_train)\n",
    "print(\"The Testing Error is:\", log_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 Regularization for Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=array([ 0.05,  0.1 , ...,  9.9 ,  9.95]),\n",
       "           class_weight=None, cv=5, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1.0, max_iter=100, multi_class='ovr',\n",
       "           n_jobs=1, penalty='l1', random_state=None, refit=True,\n",
       "           scoring=None, solver='liblinear', tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross Validation for the strength of L1 Regularization parameter C\n",
    "C_s = np.arange(0.05,10,0.05)\n",
    "lasso = LogisticRegressionCV(Cs = C_s, penalty = \"l1\", solver = \"liblinear\", cv = 5)\n",
    "lasso.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Training Error is: 0.160337552743\n",
      "The Testing Error is: 0.116666666667\n",
      "Using a regularization parameter of 0.1\n"
     ]
    }
   ],
   "source": [
    "print(\"The Training Error is:\", 1 - lasso.score(X_train,y_train))\n",
    "print(\"The Testing Error is:\", 1 - lasso.score(X_test, y_test))\n",
    "print(\"Using a regularization parameter of {}\".format(lasso.C_[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This means L1 Regularization would remove 19 variables: \n",
      " ['age' 'sex' 'rest_bp' 'chol' 'bld_gluc' 'strs_max_hr' 'ecg_st_d_exer'\n",
      " 'chest_pain_1.0' 'chest_pain_2.0' 'chest_pain_3.0' 'ecg_st_e_0.0'\n",
      " 'ecg_st_e_1.0' 'ecg_st_e_2.0' 'cor_a_1.0' 'cor_a_2.0' 'cor_a_3.0'\n",
      " 'ecg_st_slope_2.0' 'ecg_st_slope_3.0' 'strs_test_6.0']\n",
      "\n",
      " \n",
      "\n",
      "This means only keeping 6 variables:['ex_ang' 'chest_pain_4.0' 'cor_a_0.0' 'ecg_st_slope_1.0' 'strs_test_3.0'\n",
      " 'strs_test_7.0']\n"
     ]
    }
   ],
   "source": [
    "l1_var_excluded = np.array(heart_df_positive.columns)[(lasso.coef_ == 0).flatten()]\n",
    "l1_var_included = np.array(heart_df_positive.columns)[(lasso.coef_ != 0).flatten()]\n",
    "print(\"This means L1 Regularization would remove {} variables: \\n {}\".format(len(l1_var_excluded), l1_var_excluded))\n",
    "print(\"\\n\", \"\\n\")\n",
    "print(\"This means only keeping {} variables:{}\".format(len(l1_var_included), l1_var_included))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backward Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we aren't using Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=RFE(estimator=LogisticRegression(C=10000, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "  n_features_to_select=None, step=1, verbose=0),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_features_to_select': array([ 6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22,\n",
       "       23])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_features_left = np.arange(6,24,1)\n",
    "back_select = RFE(LogisticRegression(penalty=\"l2\", C = 10000))\n",
    "n_features_dict = {\"n_features_to_select\":n_features_left}\n",
    "\n",
    "rfe_grid = GridSearchCV(back_select, n_features_dict)\n",
    "rfe_grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Training Error is: 0.126582278481\n",
      "The Testing Error is: 0.116666666667\n",
      "After Cross-Validation for number of features to remain: \n",
      " \n",
      " \n",
      "There are 7 features EXCLUDED, which include \n",
      " ['ecg_st_e_1.0' 'ecg_st_e_2.0' 'cor_a_1.0' 'ecg_st_slope_2.0'\n",
      " 'ecg_st_slope_3.0' 'strs_test_3.0' 'strs_test_6.0'] \n",
      " \n",
      "\n",
      "There are 18 features SELECTED, which include \n",
      " ['age' 'sex' 'rest_bp' 'chol' 'bld_gluc' 'strs_max_hr' 'ex_ang'\n",
      " 'ecg_st_d_exer' 'chest_pain_1.0' 'chest_pain_2.0' 'chest_pain_3.0'\n",
      " 'chest_pain_4.0' 'ecg_st_e_0.0' 'cor_a_0.0' 'cor_a_2.0' 'cor_a_3.0'\n",
      " 'ecg_st_slope_1.0' 'strs_test_7.0']\n"
     ]
    }
   ],
   "source": [
    "print(\"The Training Error is:\", 1 - rfe_grid.score(X_train,y_train))\n",
    "print(\"The Testing Error is:\", 1 - rfe_grid.score(X_test, y_test))\n",
    "\n",
    "rfe_var_excluded = np.array(heart_df_positive.columns)[rfe_grid.best_estimator_.support_ != True]\n",
    "\n",
    "rfe_var_included = np.array(heart_df_positive.columns)[rfe_grid.best_estimator_.support_]\n",
    "\n",
    "\n",
    "print(\"After Cross-Validation for number of features to remain: \\n \\n \")\n",
    "\n",
    "print(\"There are {} features EXCLUDED, which include \\n {} \\n \\n\".format(len(rfe_var_excluded), rfe_var_excluded))\n",
    "\n",
    "print(\"There are {} features SELECTED, which include \\n {}\".format(len(rfe_var_included), rfe_var_included))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 6 Features Excluded by BOTH methods: \n",
      " ['cor_a_1.0' 'ecg_st_e_1.0' 'ecg_st_e_2.0' 'ecg_st_slope_2.0'\n",
      " 'ecg_st_slope_3.0' 'strs_test_6.0']\n",
      "\n",
      "\n",
      "There were 5 Features Included by BOTH methods: \n",
      " ['chest_pain_4.0' 'cor_a_0.0' 'ecg_st_slope_1.0' 'ex_ang' 'strs_test_7.0']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's combine the results from L1 Regularization and RFE\n",
    "intersect_excluded = np.intersect1d(rfe_var_excluded, l1_var_excluded)\n",
    "intersect_included = np.intersect1d(rfe_var_included, l1_var_included)\n",
    "\n",
    "print(\"There were {} Features Excluded by BOTH methods: \\n {}\\n\\n\".format(len(intersect_excluded), intersect_excluded))\n",
    "\n",
    "print(\"There were {} Features Included by BOTH methods: \\n {}\\n\\n\".format(len(intersect_included), intersect_included))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_exclude_positions= np.nonzero(lasso.coef_ == 0)[1]\n",
    "rfe_exclude_positions = np.nonzero(rfe_grid.best_estimator_.support_ != True)[0]\n",
    "\n",
    "new_exclude = np.intersect1d(l1_exclude_positions, rfe_exclude_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_include = np.setdiff1d(np.arange(0, len(heart_df_positive.columns)), new_exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[:, features_to_include]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.80, random_state = 123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature Selection Method | Training Error | Testing Error | Features Excluded | Features Included |\n",
    "| ---------------- | ------ | ------- | ------- | -------- |\n",
    "| Plain Logistic Regression | 0.12658 | 0.11667 | 0 | 25 |\n",
    "| L1 Regularization | 0.16033 | 0.11667 | 19 | 6 |\n",
    "| Recursive Feature Elimination | 0.12658 | 0.11667 | 7 | 18 |\n",
    "| Intersection of Both Methods | -- | -- | 6 | 5 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion of Feature Selection**  \n",
    "\n",
    "Firstly, the dataset that I'm using contains only 14 features of a greater 76 features. The problem is that the dataset that I've chosen has already gone through feature selection - so this makes feature selection for me much more challenging to see actual improvements with feature reduction.  \n",
    "\n",
    "Even with L1 regularization (after cross validation to select the optimal regularization parameter) the resulting test error does not actually improve compared to a model with the full feature set - but that being said it is able to achieve the same test error on logistic regression as the full feature model. Likewise Recursive Feature Elimination (after cross validation to select how many features should be elimiated) did not improve or worsen overall test error. \n",
    "\n",
    "Therefore, I chose to look at the feature excluded in *both* models and decided to remove these features when I select the model.\n",
    "\n",
    "These 6 features are:   \n",
    "'cor_a_1.0' - Having exactly 1 Coronary artery vessel coloured by fluoroscopy.  \n",
    "'ecg_st_e_1.0' - ST Elevation on ECG indicative of coronary artery involvement  \n",
    "'ecg_st_e_2.0' - ST Segment chagnes indicating Left Ventricular Hypertrophy  \n",
    "'ecg_st_slope_2.0' - ST Slope downsloping  \n",
    "'ecg_st_slope_3.0' - ST Slope flat  \n",
    "'strs_test_6.0' - ST Fixed defect  \n",
    "\n",
    "Medically speaking, some of this is surprising. Particularly the 'ecg_st_e_1.0' ST elevation finding which is a well known diagnostic criteria.  \n",
    "\n",
    "This is likely because this specific data represents patients who were able to have stress tests (i.e. they're not sick enough that they can't run on a treadmill). This means these weren't the type of patients who are coming into the emergency department with acute chest pain. This data more captures patients seen with longer term chest pain seen as outpatients. In this case it may be more reasonable to remove some of these. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': array([ 3,  5,  7,  9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35,\n",
       "       37, 39]), 'max_depth': array([1, 3, 5, 7, 9])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_forest = RandomForestClassifier()\n",
    "\n",
    "n_estimator = np.arange(3,40,2)\n",
    "max_depth = np.arange(1,10,2)\n",
    "\n",
    "rand_param = {\"n_estimators\":n_estimator, \n",
    "             \"max_depth\":max_depth}\n",
    "\n",
    "rand_grid = GridSearchCV(rand_forest, rand_param)\n",
    "rand_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 1, 'n_estimators': 19}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Training Error is: 0.172995780591\n",
      "The Testing Error is: 0.116666666667\n",
      "Using a decision stumps of 1 and 19 trees\n"
     ]
    }
   ],
   "source": [
    "print(\"The Training Error is:\", 1 - rand_grid.score(X_train,y_train))\n",
    "print(\"The Testing Error is:\", 1 - rand_grid.score(X_test, y_test))\n",
    "print(\"Using a decision stumps of {} and {} trees\".format(rand_grid.best_params_[\"max_depth\"], rand_grid.best_params_[\"n_estimators\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_neighbors': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "\n",
    "n_neighbors = np.arange(1,20,1)\n",
    "\n",
    "knn_param = {\"n_neighbors\":n_neighbors}\n",
    "\n",
    "knn_grid = GridSearchCV(knn, knn_param)\n",
    "knn_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Training Error is: 0.147679324895\n",
      "The Testing Error is: 0.15\n",
      "Using 17 Nearest Neighbors\n"
     ]
    }
   ],
   "source": [
    "print(\"The Training Error is:\", 1 - knn_grid.score(X_train,y_train))\n",
    "print(\"The Testing Error is:\", 1 - knn_grid.score(X_test, y_test))\n",
    "print(\"Using {} Nearest Neighbors\".format(knn_grid.best_params_[\"n_neighbors\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas = [10**-(float(x)) for x in range(6,-1,-1)]\n",
    "C = gammas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'gamma': [1e-06, 1e-05, 0.0001, 0.001, 0.01, 0.1, 1.0], 'C': [1e-06, 1e-05, 0.0001, 0.001, 0.01, 0.1, 1.0]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = SVC()\n",
    "svm_param = {\"gamma\": gammas, \n",
    "            \"C\": C}\n",
    "\n",
    "svm_grid = GridSearchCV(svm, svm_param)\n",
    "svm_grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0, 'gamma': 0.01}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Training Error is: 0.147679324895\n",
      "The Testing Error is: 0.166666666667\n",
      "Using C: 1.0, gamma: 0.01\n"
     ]
    }
   ],
   "source": [
    "1 - svm_grid.score(X_test,y_test)\n",
    "print(\"The Training Error is:\", 1 - svm_grid.score(X_train,y_train))\n",
    "print(\"The Testing Error is:\", 1 - svm_grid.score(X_test,y_test))\n",
    "print(\"Using C: {}, gamma: {}\".format(svm_grid.best_params_[\"C\"], svm_grid.best_params_[\"gamma\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Training Error is: 0.160337552743\n",
      "The Testing Error is: 0.116666666667\n",
      "Using C: 0.1\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(penalty = \"l1\", C = 0.1)\n",
    "log_reg.fit(X_train,y_train)\n",
    "\n",
    "print(\"The Training Error is:\", 1 - log_reg.score(X_train,y_train))\n",
    "print(\"The Testing Error is:\", 1 - log_reg.score(X_test,y_test))\n",
    "print(\"Using C: {}\".format(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 6 Features that were Non-Zero: \n",
      " [ 6 11 13 16 17 18] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"There were {} Features that were Non-Zero: \\n {} \\n\".format(len(np.nonzero(log_reg.coef_)[1]), np.nonzero(log_reg.coef_)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | Training Error | Testing Error |\n",
    "| ----- | ------------ | ------------- |\n",
    "| Random Forest | 0.17300 | 0.15 |\n",
    "| KNN | 0.14768 | 0.15 |\n",
    "| SVM | 0.14768 | 0.16667 | \n",
    "| Logistic Regression | 0.16033 | 0.11666 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why did this happen? In the feature selection step, we couldn't improve the basic logistic regression model even when we removed features, and then in the model selection step, all our models were actually *worse* than the CV L1 regularized logistic regression model. What gives?  \n",
    "\n",
    "I think I learned a few lessons with this analysis. \n",
    "\n",
    "Firstly, these results are a consequence of using a dataset that has already undergone feature selection. As I mentioned above there are 14 features in this dataset out of a greater 76 original features. When I decided to reduce these features further, in every step I used cross-validation (and thus didn't report the validation error, but rather the training error of the model that was selected by cross-validation). As mentioned in the previous section, each feature selection step didn't actually improve the overall test error (and note - I kept the test set separate during cross-validation to avoid overfitting). With this in mind, it is not overly surprising that the model selection phase didn't work very well. \n",
    "\n",
    "When we did make it to model selection, the best test error we could get was with logistic regression that only kept 6 features (after L1 regularization) and returned the same test error as the *original logistic regression model with all features and without regularization*. This is because the dataset I was using had already been feature selected for logistic regression before I even began. Unfortunately, despite further subsetting features and trying out different models, I was still unable to reduce the test error that I started with. \n",
    "\n",
    "While I was feature selecting (and although I was cross-validating and not tuning hyperparameters on the test set which I know is a big no-no), I did look at the result in the context of how it performed on test data at each step. When I was feature selecting and cross-validating for L1 regularization and backward selection, both of these steps used Logistic Regression as a base estimator, and thus I feature selected in such a way as to minimize error in logistic regression. Perhaps, I should have feature selected with different estimators in my cross-validation (for instance using KNN in the backward selection) and I would have gotten features that would work better in KNN.  \n",
    "\n",
    "Additional future directions could include trying additional ensemble methods than Random Forest, such as bagging, to see if I can improve the overall model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
